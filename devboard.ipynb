{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetworkDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self,imageFolderDataset,transform=None,should_invert=True):\n",
    "        self.imageFolderDataset = imageFolderDataset    \n",
    "        self.transform = transform\n",
    "        self.should_invert = should_invert\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        img0_tuple = random.choice(self.imageFolderDataset.imgs)\n",
    "        #we need to make sure approx 50% of images are in the same class\n",
    "        should_get_same_class = random.randint(0,1) \n",
    "        if should_get_same_class:\n",
    "            while True:\n",
    "                #keep looping till the same class image is found\n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                if img0_tuple[1]==img1_tuple[1]:\n",
    "                    break\n",
    "        else:\n",
    "            img1_tuple = random.choice(self.imageFolderDataset.imgs)\n",
    "        \n",
    "#         print(img0_tuple, img1_tuple)\n",
    "        img0 = Image.open(img0_tuple[0])\n",
    "        img1 = Image.open(img1_tuple[0])\n",
    "        img0 = img0.convert(\"RGB\")\n",
    "        img1 = img1.convert(\"RGB\")\n",
    "        \n",
    "        if self.should_invert:\n",
    "            img0 = PIL.ImageOps.invert(img0)\n",
    "            img1 = PIL.ImageOps.invert(img1)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img0 = self.transform(img0)\n",
    "            img1 = self.transform(img1)\n",
    "        \n",
    "        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1]!=img0_tuple[1])],dtype=np.float32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imageFolderDataset.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_EPOCH = 10\n",
    "LRATE = 0.001\n",
    "MOMENTUM = 0.9\n",
    "BETA1 = 0.5\n",
    "BETA2 = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path  = 'dataset/train'\n",
    "valid_path  = 'dataset/valid'\n",
    "\n",
    "\n",
    "tmft_train = transforms.Compose([\n",
    "    transforms.Resize((112,112)),\n",
    "#     transforms.RandomRotation(30),\n",
    "    transforms.CenterCrop((112)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "tmft_valid = transforms.Compose([\n",
    "    transforms.Resize((112,112)),\n",
    "    transforms.CenterCrop((112)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset_imagefolder = datasets.ImageFolder(train_path)\n",
    "trainset = SiameseNetworkDataset(trainset_imagefolder, transform=tmft_train)\n",
    "trainloader = data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "valid_imagefolder = datasets.ImageFolder(valid_path)\n",
    "validset = SiameseNetworkDataset(valid_imagefolder, transform=tmft_valid)\n",
    "validloader = data.DataLoader(validset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'op' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5689fcbd5573>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mim1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'op' is not defined"
     ]
    }
   ],
   "source": [
    "# plt.imshow(im1[0].permute(1,2,0));plt.show()\n",
    "# plt.imshow(im2[0].permute(1,2,0));plt.show()\n",
    "# print(sim[0])\n",
    "im1, im2, sim = next(iter(trainloader))\n",
    "\n",
    "op.zero_grad()\n",
    "out1, out2 = net.forward(im1, im2)\n",
    "loss = cr(out1, out2, sim)\n",
    "loss.backward()\n",
    "op.step()\n",
    "\n",
    "out1, out2 = net.forward(im1, im2)\n",
    "print(net.cnn1[1].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plt.imshow(im1[0].permute(1,2,0));plt.show()\n",
    "# # plt.imshow(im2[0].permute(1,2,0));plt.show()\n",
    "# # print(sim[0])\n",
    "\n",
    "# plt.imshow(im1[0].squeeze());plt.show()\n",
    "# plt.imshow(im2[0].squeeze());plt.show()\n",
    "# print(sim[0])\n",
    "\n",
    "# # im1.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_chan, out_chan, kernel_size=3, padding=1, stride=1, use_batchnorm=True, use_dropout=False):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_chan, out_chan, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        if use_batchnorm:\n",
    "            self.bn = nn.BatchNorm2d(out_chan)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        if use_dropout:\n",
    "            self.dropout = nn.Dropout(p=0.3)\n",
    "        \n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.use_dropout = use_dropout\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.use_batchnorm:\n",
    "            x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        if self.use_dropout:\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ConvBlock(1,32),\n",
    "            ConvBlock(32,32),\n",
    "            nn.MaxPool2d(2,2),\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            ConvBlock(32,64),\n",
    "            ConvBlock(64,64),\n",
    "            ConvBlock(64,64),\n",
    "            nn.MaxPool2d(2,2),\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            ConvBlock(64,128),\n",
    "            ConvBlock(128,128),\n",
    "            ConvBlock(128,128),\n",
    "            nn.MaxPool2d(2,2),\n",
    "        )\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            ConvBlock(128,256),\n",
    "            ConvBlock(256,256),\n",
    "            ConvBlock(256,256),\n",
    "            nn.MaxPool2d(2,2),\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(256*7*7),\n",
    "            nn.Linear(256*7*7, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            \n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            \n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Linear(128, 32)\n",
    "        )\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2\n",
    "    \n",
    "    \n",
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        if torch.cuda.is_available():\n",
    "            euclidean_distance = euclidean_distance.cpu()\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LRATE, momentum=MOMENTUM, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img1, img2, label = next(iter(trainloader))\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     img1 = img1.cuda()\n",
    "#     img2 = img2.cuda()\n",
    "\n",
    "# optimizer.zero_grad()\n",
    "# output1, output2 = model.forward(img1, img2) #forward prop\n",
    "# loss = criterion(output1, output2, label)\n",
    "# loss.backward()\n",
    "# optimizer.step()\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \tTrain Loss: 1.235878825187683\n",
      "Epoch 0 \tTrain Loss: 1.4618456363677979\n",
      "Epoch 0 \tTrain Loss: 1.4081320762634277\n",
      "Epoch 0 \tTrain Loss: 1.1728777885437012\n",
      "Epoch 0 \tTrain Loss: 1.5897942781448364\n",
      "Epoch 0 \tTrain Loss: 1.4390064477920532\n",
      "Epoch 0 \tTrain Loss: 1.1586263179779053\n",
      "Epoch 0 \tTrain Loss: 1.132370948791504\n",
      "Epoch 0 \tTrain Loss: 1.0924570560455322\n",
      "Epoch 0 \tValid Loss: 2.363787889480591\n",
      "Epoch 0 \tValid Loss: 1.4935989379882812\n",
      "Epoch 0 \tValid Loss: 1.493749976158142\n",
      "Epoch 0 \tValid Loss: 1.4937816858291626\n",
      "Epoch 0 \tValid Loss: 1.493717908859253\n",
      "Epoch 0 \tValid Loss: 1.7431414127349854\n",
      "Epoch 0 \tValid Loss: 1.4942411184310913\n",
      "Epoch 0 \tValid Loss: 1.6190800666809082\n",
      "Epoch 0 \tValid Loss: 2.6549291610717773\n",
      "Epoch 1 \tTrain Loss: 1.1930712461471558\n",
      "Epoch 1 \tTrain Loss: 1.2031035423278809\n",
      "Epoch 1 \tTrain Loss: 1.286432147026062\n",
      "Epoch 1 \tTrain Loss: 1.1352452039718628\n",
      "Epoch 1 \tTrain Loss: 1.1941673755645752\n",
      "Epoch 1 \tTrain Loss: 1.1804182529449463\n",
      "Epoch 1 \tTrain Loss: 1.3050987720489502\n",
      "Epoch 1 \tTrain Loss: 1.121098279953003\n",
      "Epoch 1 \tTrain Loss: 1.2098640203475952\n",
      "Epoch 1 \tValid Loss: 1.546378493309021\n",
      "Epoch 1 \tValid Loss: 2.2252821922302246\n",
      "Epoch 1 \tValid Loss: 2.4764621257781982\n",
      "Epoch 1 \tValid Loss: 1.8964067697525024\n",
      "Epoch 1 \tValid Loss: 2.014991283416748\n",
      "Epoch 1 \tValid Loss: 2.0126798152923584\n",
      "Epoch 1 \tValid Loss: 1.3096033334732056\n",
      "Epoch 1 \tValid Loss: 2.243671178817749\n",
      "Epoch 1 \tValid Loss: 3.137539863586426\n",
      "Epoch 2 \tTrain Loss: 1.10309636592865\n",
      "Epoch 2 \tTrain Loss: 1.2376281023025513\n",
      "Epoch 2 \tTrain Loss: 1.1212632656097412\n",
      "Epoch 2 \tTrain Loss: 1.3081345558166504\n",
      "Epoch 2 \tTrain Loss: 1.1609861850738525\n",
      "Epoch 2 \tTrain Loss: 1.154745101928711\n",
      "Epoch 2 \tTrain Loss: 1.0369946956634521\n",
      "Epoch 2 \tTrain Loss: 1.2238998413085938\n",
      "Epoch 2 \tTrain Loss: 1.0370270013809204\n",
      "Epoch 2 \tValid Loss: 1.7640507221221924\n",
      "Epoch 2 \tValid Loss: 1.9120094776153564\n",
      "Epoch 2 \tValid Loss: 1.6037286520004272\n",
      "Epoch 2 \tValid Loss: 1.688340425491333\n",
      "Epoch 2 \tValid Loss: 2.1522274017333984\n",
      "Epoch 2 \tValid Loss: 1.671570897102356\n",
      "Epoch 2 \tValid Loss: 1.5063215494155884\n",
      "Epoch 2 \tValid Loss: 1.3712083101272583\n",
      "Epoch 2 \tValid Loss: 1.6823554039001465\n",
      "Epoch 3 \tTrain Loss: 1.1052900552749634\n",
      "Epoch 3 \tTrain Loss: 1.134157419204712\n",
      "Epoch 3 \tTrain Loss: 1.0354732275009155\n",
      "Epoch 3 \tTrain Loss: 1.1836192607879639\n",
      "Epoch 3 \tTrain Loss: 1.0289149284362793\n",
      "Epoch 3 \tTrain Loss: 1.1181046962738037\n",
      "Epoch 3 \tTrain Loss: 1.0963412523269653\n",
      "Epoch 3 \tTrain Loss: 1.0705134868621826\n",
      "Epoch 3 \tTrain Loss: 1.2748689651489258\n",
      "Epoch 3 \tValid Loss: 1.0163829326629639\n",
      "Epoch 3 \tValid Loss: 1.354384422302246\n",
      "Epoch 3 \tValid Loss: 1.4822754859924316\n",
      "Epoch 3 \tValid Loss: 1.5326635837554932\n",
      "Epoch 3 \tValid Loss: 1.28289794921875\n",
      "Epoch 3 \tValid Loss: 1.4869146347045898\n",
      "Epoch 3 \tValid Loss: 1.5608632564544678\n",
      "Epoch 3 \tValid Loss: 1.7938417196273804\n",
      "Epoch 3 \tValid Loss: 1.182780146598816\n",
      "Epoch 4 \tTrain Loss: 1.0979175567626953\n",
      "Epoch 4 \tTrain Loss: 1.2503455877304077\n",
      "Epoch 4 \tTrain Loss: 1.0800747871398926\n",
      "Epoch 4 \tTrain Loss: 1.1663473844528198\n",
      "Epoch 4 \tTrain Loss: 1.0927969217300415\n",
      "Epoch 4 \tTrain Loss: 1.1303186416625977\n",
      "Epoch 4 \tTrain Loss: 1.0628290176391602\n",
      "Epoch 4 \tTrain Loss: 1.0769269466400146\n",
      "Epoch 4 \tTrain Loss: 1.186002254486084\n",
      "Epoch 4 \tValid Loss: 1.1121546030044556\n",
      "Epoch 4 \tValid Loss: 1.3932757377624512\n",
      "Epoch 4 \tValid Loss: 1.5800756216049194\n",
      "Epoch 4 \tValid Loss: 1.3976346254348755\n",
      "Epoch 4 \tValid Loss: 1.118977427482605\n",
      "Epoch 4 \tValid Loss: 1.3516647815704346\n",
      "Epoch 4 \tValid Loss: 1.5735465288162231\n",
      "Epoch 4 \tValid Loss: 1.6932084560394287\n",
      "Epoch 4 \tValid Loss: 1.1961501836776733\n",
      "Epoch 5 \tTrain Loss: 1.0269172191619873\n",
      "Epoch 5 \tTrain Loss: 1.041275978088379\n",
      "Epoch 5 \tTrain Loss: 1.0931172370910645\n",
      "Epoch 5 \tTrain Loss: 1.190272569656372\n",
      "Epoch 5 \tTrain Loss: 1.1209115982055664\n",
      "Epoch 5 \tTrain Loss: 0.9950451850891113\n",
      "Epoch 5 \tTrain Loss: 1.0851970911026\n",
      "Epoch 5 \tTrain Loss: 1.133988380432129\n",
      "Epoch 5 \tTrain Loss: 1.063854694366455\n",
      "Epoch 5 \tValid Loss: 1.416532278060913\n",
      "Epoch 5 \tValid Loss: 1.5137358903884888\n",
      "Epoch 5 \tValid Loss: 1.3301517963409424\n",
      "Epoch 5 \tValid Loss: 1.3127838373184204\n",
      "Epoch 5 \tValid Loss: 1.0254179239273071\n",
      "Epoch 5 \tValid Loss: 0.9565505981445312\n",
      "Epoch 5 \tValid Loss: 1.5029029846191406\n",
      "Epoch 5 \tValid Loss: 1.588148593902588\n",
      "Epoch 5 \tValid Loss: 1.7172154188156128\n",
      "Epoch 6 \tTrain Loss: 1.1149556636810303\n",
      "Epoch 6 \tTrain Loss: 1.1420090198516846\n",
      "Epoch 6 \tTrain Loss: 1.0510945320129395\n",
      "Epoch 6 \tTrain Loss: 1.0976061820983887\n",
      "Epoch 6 \tTrain Loss: 1.1518653631210327\n",
      "Epoch 6 \tTrain Loss: 1.146587610244751\n",
      "Epoch 6 \tTrain Loss: 1.083531141281128\n",
      "Epoch 6 \tTrain Loss: 1.0421264171600342\n",
      "Epoch 6 \tTrain Loss: 1.2250999212265015\n",
      "Epoch 6 \tValid Loss: 1.3384745121002197\n",
      "Epoch 6 \tValid Loss: 1.3067047595977783\n",
      "Epoch 6 \tValid Loss: 1.1041646003723145\n",
      "Epoch 6 \tValid Loss: 1.3281182050704956\n",
      "Epoch 6 \tValid Loss: 1.057605504989624\n",
      "Epoch 6 \tValid Loss: 1.2087187767028809\n",
      "Epoch 6 \tValid Loss: 1.077759027481079\n",
      "Epoch 6 \tValid Loss: 0.9596205353736877\n",
      "Epoch 6 \tValid Loss: 1.0937618017196655\n",
      "Epoch 7 \tTrain Loss: 1.0316593647003174\n",
      "Epoch 7 \tTrain Loss: 0.9881594181060791\n",
      "Epoch 7 \tTrain Loss: 1.0108836889266968\n",
      "Epoch 7 \tTrain Loss: 0.9423981308937073\n",
      "Epoch 7 \tTrain Loss: 1.0662566423416138\n",
      "Epoch 7 \tTrain Loss: 1.0419694185256958\n",
      "Epoch 7 \tTrain Loss: 1.2373238801956177\n",
      "Epoch 7 \tTrain Loss: 1.0777547359466553\n",
      "Epoch 7 \tTrain Loss: 1.145774245262146\n",
      "Epoch 7 \tValid Loss: 1.1373260021209717\n",
      "Epoch 7 \tValid Loss: 1.3280106782913208\n",
      "Epoch 7 \tValid Loss: 1.5000758171081543\n",
      "Epoch 7 \tValid Loss: 1.0851086378097534\n",
      "Epoch 7 \tValid Loss: 1.6565945148468018\n",
      "Epoch 7 \tValid Loss: 1.5129427909851074\n",
      "Epoch 7 \tValid Loss: 1.3697515726089478\n",
      "Epoch 7 \tValid Loss: 1.4850997924804688\n",
      "Epoch 7 \tValid Loss: 1.430885910987854\n",
      "Epoch 8 \tTrain Loss: 1.1328630447387695\n",
      "Epoch 8 \tTrain Loss: 1.0801335573196411\n",
      "Epoch 8 \tTrain Loss: 1.1407698392868042\n",
      "Epoch 8 \tTrain Loss: 1.0409854650497437\n",
      "Epoch 8 \tTrain Loss: 1.1679449081420898\n",
      "Epoch 8 \tTrain Loss: 1.0551438331604004\n",
      "Epoch 8 \tTrain Loss: 1.1274874210357666\n",
      "Epoch 8 \tTrain Loss: 1.096976637840271\n",
      "Epoch 8 \tTrain Loss: 1.0599416494369507\n",
      "Epoch 8 \tValid Loss: 1.0897655487060547\n",
      "Epoch 8 \tValid Loss: 1.219399094581604\n",
      "Epoch 8 \tValid Loss: 1.082368016242981\n",
      "Epoch 8 \tValid Loss: 1.251367449760437\n",
      "Epoch 8 \tValid Loss: 1.1297926902770996\n",
      "Epoch 8 \tValid Loss: 1.2497787475585938\n",
      "Epoch 8 \tValid Loss: 0.9452185034751892\n",
      "Epoch 8 \tValid Loss: 1.3958487510681152\n",
      "Epoch 8 \tValid Loss: 1.248961329460144\n",
      "Epoch 9 \tTrain Loss: 1.0652964115142822\n",
      "Epoch 9 \tTrain Loss: 1.042052149772644\n",
      "Epoch 9 \tTrain Loss: 1.0814695358276367\n",
      "Epoch 9 \tTrain Loss: 1.131416916847229\n",
      "Epoch 9 \tTrain Loss: 1.1349555253982544\n",
      "Epoch 9 \tTrain Loss: 1.0264188051223755\n",
      "Epoch 9 \tTrain Loss: 1.0787358283996582\n",
      "Epoch 9 \tTrain Loss: 1.0399255752563477\n",
      "Epoch 9 \tTrain Loss: 1.028632640838623\n",
      "Epoch 9 \tValid Loss: 1.3271737098693848\n",
      "Epoch 9 \tValid Loss: 1.1443537473678589\n",
      "Epoch 9 \tValid Loss: 1.1088359355926514\n",
      "Epoch 9 \tValid Loss: 1.601319432258606\n",
      "Epoch 9 \tValid Loss: 1.3153892755508423\n",
      "Epoch 9 \tValid Loss: 1.3279818296432495\n",
      "Epoch 9 \tValid Loss: 1.6417276859283447\n",
      "Epoch 9 \tValid Loss: 1.38776695728302\n",
      "Epoch 9 \tValid Loss: 0.9749746322631836\n"
     ]
    }
   ],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, num_epoch, model, dataloader, criterion, optimizer):\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    for idx, (img1,img2,label) in enumerate(dataloader):\n",
    "        data_time.update(time.time() - end_time)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            img1 = img1.cuda()\n",
    "            img2 = img2.cuda()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        output1, output2 = model.forward(img1, img2) #forward prop\n",
    "        loss = criterion(output1, output2, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.update(loss.item(), img1.size(0))\n",
    "        batch_time.update(time.time() - end_time)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        if idx % 5==0:\n",
    "            print(f'Train Epoch [{epoch+1}/{num_epoch}] [{idx}/{len(dataloader)}]\\t'\n",
    "                  f' Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  f' Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  f' Loss {losses.val:.4f} ({losses.avg:.4f}) ')\n",
    "        \n",
    "    return losses.avg\n",
    "        \n",
    "def valid(epoch, num_epoch, model, dataloader, criterion, optimizer):\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        end_time = time.time()\n",
    "        for idx, (img1,img2,label) in enumerate(dataloader):\n",
    "            data_time.update(time.time() - end_time)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                img1 = img1.cuda()\n",
    "                img2 = img2.cuda()\n",
    "\n",
    "            output1, output2 = model.forward(img1, img2) #forward prop\n",
    "            loss = criterion(output1, output2, label)\n",
    "            \n",
    "            losses.update(loss.item(), img1.size(0))\n",
    "            batch_time.update(time.time() - end_time)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            if idx % 5==0:\n",
    "                print(f'Valid Epoch [{epoch + 1}/{num_epoch}] [{idx}/{len(dataloader)}]\\t'\n",
    "                      f' Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      f' Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                      f' Loss {losses.val:.4f} ({losses.avg:.4f}) ')\n",
    "            \n",
    "    return losses.avg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'train':[], 'valid':[]}\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    trainloss = train(epoch, NUM_EPOCH, model, trainloader, criterion, optimizer)\n",
    "    validloss = valid(epoch, NUM_EPOCH, model, validloader, criterion, optimizer)\n",
    "    history['train'].append(trainloss)\n",
    "    history['valid'].append(validloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = list(range(len(history['train'])))\n",
    "plt.plot(index, history['train'], label='train_loss')\n",
    "plt.plot(index, history['valid'], label='valid_loss')\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1, im2, label = next(iter(validloader))\n",
    "if torch.cuda.is_available():\n",
    "    im1 = im1.cuda()\n",
    "    im2 = im2.cuda()\n",
    "out1, out2 = model.forward(im1,im2)\n",
    "predicted_label = F.pairwise_distance(out1, out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = model.layer1(im1)\n",
    "layer2 = model.layer2(layer1)\n",
    "layer3 = model.layer3(layer2)\n",
    "layer4 = model.layer4(layer3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(layer2[0][32].cpu().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(val, threshold=0.5):\n",
    "    if val>threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "fn, fp, ap, an = 0,0,0,0\n",
    "for idx, (img1,img2,label) in enumerate(validloader):\n",
    "    if torch.cuda.is_available():\n",
    "        img1 = img1.cuda()\n",
    "        img2 = img2.cuda()\n",
    "    out1, out2 = model.forward(img1,img2)\n",
    "    predicted_label = F.pairwise_distance(out1, out2)\n",
    "    \n",
    "    for i in range(img1.size(0)):\n",
    "        pr = step(predicted_label[i].item(), threshold=0.3)\n",
    "        gt = int(label[i].item())\n",
    "        \n",
    "        if pr==False and gt==True: fn+=1\n",
    "        elif pr==True and gt==True: ap+=1\n",
    "        elif pr==False and gt==False: an+=1\n",
    "        elif pr==True and gt==False: fp+=1\n",
    "        \n",
    "fn, fp, ap, an\n",
    "error = (fn + fp) / (fp+fn+ap+an) * 100\n",
    "accuracy = ap+an / (fp+fn+ap+an) * 100\n",
    "accuracy, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1==True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3\n",
    "i1 = im1[idx]\n",
    "i2 = im2[idx]\n",
    "lbl = label[idx]\n",
    "pl = threshold(predicted_label[idx].item())\n",
    "\n",
    "print(f'Predicted: {pl}\\t Ground Truth: {lbl.item()}')\n",
    "plt.imshow(i1.squeeze().cpu(), cmap='gray');plt.show()\n",
    "plt.imshow(i2.squeeze().cpu(), cmap='gray');plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validset[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
