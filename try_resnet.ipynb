{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import pathlib\n",
    "from torch.utils import data\n",
    "from siamese_dataset_example import SiamesePairDataset\n",
    "import PIL.Image as Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import pandas\n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "from models.siamese_resnet import SiameseResnet\n",
    "import log_compiler\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive Loss\n",
    "This function used to calculate the loss/cost of our input image, based on this paper by Yan Lecun http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf. Because siamese is not classification problem rather a distance problem which means we need to compute the difference between two images hence we need another type of loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, device, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        euclidean_distance = euclidean_distance.to(self.device)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamesse Network class, based on Resnet18 model\n",
    "We are using resnet18 model with custom fully connected layer based on our very case. But first, we need to freeze layer parameters so they dont get recalculated on the training. The only layer we are training is last layer which is the fully connected layer that has been customized to our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few setups before training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rounding(val, threshold=0.5):\n",
    "    return 1 if val > threshold else 0\n",
    "\n",
    "def log_training_result(numepoch, batchsize, lrate, accuracy, precision, f1, tp, tn, fp, fn, fc, model, name='training_logs.csv'):\n",
    "    with open(name, 'r') as f:\n",
    "        train_number = len(f.readlines())\n",
    "    detail = log_compiler.compile(numepoch, batchsize, lrate, accuracy, precision, f1, tp, tn, fp, fn, fc, train_number, model)\n",
    "    data = [batchsize,lrate,numepoch,round(accuracy, 2),round(f1, 2),round(precision, 2),tp,tn,fp,fn,f'logs/{train_number}.md']\n",
    "    log_compiler.write_log_file(train_number, detail)\n",
    "    with open(name, 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(data)\n",
    "    print(\"data saved to %s\" % name)\n",
    "    \n",
    "def print_scores(p, r, f1, a, batch_size):\n",
    "    # just an utility printing function\n",
    "    for name, scores in zip((\"precision\", \"recall\", \"F1\", \"accuracy\"), (p, r, f1, a)):\n",
    "        print(f\"\\t{name.rjust(14, ' ')}: {sum(scores)/batch_size:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(epoch, trainloader, testloader, model, criterion, optimizer):\n",
    "    running_loss = 0\n",
    "    progress = tqdm(enumerate(trainloader, 1), desc=\"loss: \", total=len(trainloader))\n",
    "    for steps, (imgs1, imgs2, labels) in progress:\n",
    "        imgs1, imgs2, labels = imgs1.to(device), imgs2.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        oimgs1, oimgs2 = model.forward(imgs1, imgs2)\n",
    "        loss = criterion(oimgs1, oimgs2, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        progress.set_description(f\"Loss: {loss.item():.3f}\")\n",
    "\n",
    "        # only validate test data on last train iteration\n",
    "        if steps == len(trainloader):\n",
    "            test_loss = 0\n",
    "            accuracy, precision, recall, f1 = [], [], [], []\n",
    "            \n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for counter, (timgs1, timgs2, tlabels) in enumerate(testloader, 1):\n",
    "                    timgs1, timgs2, tlabels = timgs1.to(device), timgs2.to(device), tlabels.to(device)\n",
    "                    toimgs1, toimgs2 = model.forward(timgs1, timgs2)\n",
    "                    batch_loss = criterion(toimgs1, toimgs2, labels)\n",
    "                    test_loss += batch_loss.item()\n",
    "\n",
    "\n",
    "                    predicted_label = F.pairwise_distance(toimgs1, toimgs2)\n",
    "                    y_pred = list(map(rounding, predicted_label))\n",
    "                    y_true = list(map(int,tlabels.view(-1).cpu().numpy()))\n",
    "                    accuracy.append(metrics.accuracy_score(y_true, y_pred))\n",
    "                    precision.append(metrics.precision_score(y_true, y_pred))\n",
    "                    recall.append(metrics.recall_score(y_true, y_pred))\n",
    "                    f1.append(metrics.f1_score(y_true, y_pred))\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{NUM_EPOCH}, training loss: {running_loss/len(trainloader):.3f}, validation loss: {test_loss/len(testloader):.3f}\")\n",
    "            print_scores(precision, recall, f1, accuracy, len(testloader))\n",
    "            train_losses.append(running_loss/len(trainloader))\n",
    "            test_losses.append(test_loss/len(testloader))\n",
    "            running_loss = 0\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters\n",
    "A few arbitrary predefined parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCH = 10\n",
    "ARCHITECTURE = 'resnet50'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'dataset/train/yale'\n",
    "valid_path = 'dataset/valid/yale'\n",
    "\n",
    "trfrm = transforms.Compose([\n",
    "    lambda img: img.convert(\"RGB\"),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.CenterCrop((224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_set = SiamesePairDataset(root=train_path, ext='jpg', transform=trfrm)\n",
    "valid_set = SiamesePairDataset(root=valid_path, ext='jpg', transform=trfrm)\n",
    "train_loader = data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = data.DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (1): Dropout(p=0.5)\n",
      "  (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (3): ReLU(inplace)\n",
      "  (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): Dropout(p=0.5)\n",
      "  (6): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (7): ReLU(inplace)\n",
      "  (8): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (9): Dropout(p=0.5)\n",
      "  (10): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "fc = lambda in_features: nn.Sequential(\n",
    "    nn.BatchNorm1d(in_features),\n",
    "    nn.Dropout(.5),\n",
    "    nn.Linear(in_features, in_features//2),\n",
    "    nn.ReLU(inplace=True),\n",
    "    \n",
    "    nn.BatchNorm1d(in_features//2),\n",
    "    nn.Dropout(.5),\n",
    "    nn.Linear(in_features//2, in_features//4),\n",
    "    nn.ReLU(inplace=True),\n",
    "    \n",
    "    nn.BatchNorm1d(in_features//4),\n",
    "    nn.Dropout(.5),\n",
    "    nn.Linear(in_features//4, 128),\n",
    ")\n",
    "\n",
    "model = SiameseResnet(architecture=ARCHITECTURE, fc_layer=fc).to(device)\n",
    "criterion = ContrastiveLoss(device=device)\n",
    "optimizer = optim.Adam(model.model.fc.parameters(), lr=LEARNING_RATE)\n",
    "print(model.model.fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train only FC layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844b4d8eee34477cbaa068af4a2035a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='loss: ', max=262, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, training loss: 1.584, validation loss: 1.471\n",
      "\t     precision: 0.6433\n",
      "\t        recall: 0.0556\n",
      "\t            F1: 0.1018\n",
      "\t      accuracy: 0.5119\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537a8b8b67aa423d88f931a84fdaef18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='loss: ', max=262, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_losses, test_losses = [], []\n",
    "start_ts = time.time()\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    train_and_validate(epoch=epoch, trainloader=train_loader, criterion=criterion, optimizer=optimizer, testloader=valid_loader, model=model)\n",
    "print(f\"Training time: {time.time()-start_ts}s\")\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(test_losses, label='valid')\n",
    "plt.legend(frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3883f3c8b82846cba0c9386a5efe51ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "labels = []\n",
    "preds = []\n",
    "progress = tqdm(enumerate(valid_loader, 1))\n",
    "len_valid = len(valid_loader)\n",
    "for idx, (img1,img2,label) in progress:\n",
    "    progress.set_description(f'{idx}/{len_valid}')\n",
    "    img1 = img1.to(device)\n",
    "    img2 = img2.to(device)\n",
    "    out1, out2 = model.forward(img1,img2)\n",
    "    predicted_label = F.pairwise_distance(out1, out2)\n",
    "    rounded = list(map(rounding, predicted_label))\n",
    "    preds.append(rounded)\n",
    "    labels.append(list(map(int, label.view(-1))))\n",
    "    \n",
    "y_true = np.array(list(itertools.chain(*labels)))\n",
    "y_pred = np.array(list(itertools.chain(*preds)))\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred).ravel()\n",
    "total = tn+tp+fp+fn\n",
    "print(f'Predicted true and actually true: {tp}'\n",
    "      f'\\nPredicted false and actually false: {tn}'\n",
    "      f'\\nPredicted true but actually false: {fp}'\n",
    "      f'\\nPredicted false but actually true: {fn}'\n",
    "      f'\\nTotal correct predictions: {tp+tn} ({(tp+tn)/total*100:.2f})'\n",
    "      f'\\nTotal wrong predictions: {fn+fp} ({(fn+fp)/total*100:.2f})'\n",
    "      f'\\nTotal: ({total})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = metrics.accuracy_score(y_true, y_pred)*100\n",
    "f1 = metrics.f1_score(y_true, y_pred)*100\n",
    "prec = metrics.precision_score(y_true, y_pred)*100\n",
    "print(f'Batch Size: {BATCH_SIZE}\\t Learning Rate: {LEARNING_RATE}\\t NUM EPOCH: {NUM_EPOCH}')\n",
    "print(f'Accuracy: {acc:.2f}\\t F1: {f1:.2f}\\t Precision: {prec:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for training_logs.csv purpose\n",
    "log_training_result(NUM_EPOCH, BATCH_SIZE, LEARNING_RATE, acc, prec, f1, tp, tn, fp, fn, fc, ARCHITECTURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_csv(\"training_logs.csv\")\n",
    "df[-15:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set = SiamesePairDataset(root='dataset/yale', ext='jpg', transform=trfrm)\n",
    "# test_loader = data.DataLoader(test_set, batch_size=4000, shuffle=True)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# labels = []\n",
    "# preds = []\n",
    "# progress = tqdm(enumerate(test_loader), desc=\"Testing \")\n",
    "# for idx, (img1,img2,label) in progress:\n",
    "#     progress.set_description(f\"Testing {idx+1}/{len(test_loader)}\")\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.empty_cache()\n",
    "#     img1 = img1.to(device)\n",
    "#     img2 = img2.to(device)\n",
    "#     out1, out2 = model.forward(img1,img2)\n",
    "#     predicted_label = F.pairwise_distance(out1, out2)\n",
    "#     rounded = list(map(rounding, predicted_label))\n",
    "#     preds.append(rounded)\n",
    "#     labels.append(list(map(int, label.view(-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = np.array(list(itertools.chain(*labels)))\n",
    "# y_pred = np.array(list(itertools.chain(*preds)))\n",
    "# tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred).ravel()\n",
    "# total = tn+tp+fp+fn\n",
    "# print(\"Testing results:\")\n",
    "# print(f'Predicted true and actually true: {tp}'\n",
    "#       f'\\nPredicted false and actually false: {tn}'\n",
    "#       f'\\nPredicted true but actually false: {fp}'\n",
    "#       f'\\nPredicted false but actually true: {fn}'\n",
    "#       f'\\nTotal correct predictions: {tp+tn} ({(tp+tn)/total*100:.2f})'\n",
    "#       f'\\nTotal wrong predictions: {fn+fp} ({(fn+fp)/total*100:.2f})'\n",
    "#       f'\\nTotal: ({total})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x0,x1,label = next(iter(test_loader))\n",
    "# x0,x1,label = x0.to(device), x1.to(device), label.to(device)\n",
    "\n",
    "# o1, o2 = model.forward(x0, x1)\n",
    "# sim = F.pairwise_distance(o1,o2)\n",
    "\n",
    "# emoji = \"✅\" if int(label[0].item()) == int(rounding(sim[0].item())) else \"❌\"\n",
    "# print(f\"{emoji} Truth: {int(label[0].item())}\", f\"Pred: {rounding(sim[0].item())}\")\n",
    "\n",
    "# topil = transforms.ToPILImage()\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.imshow(torch.squeeze(x0[0].cpu()).permute(1,2,0))\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.imshow(torch.squeeze(x1[0].cpu()).permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# models.resnet152().fc.in_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
