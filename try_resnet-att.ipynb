{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import pathlib\n",
    "from torch.utils import data\n",
    "from siamese_dataset_example import SiamesePairDataset\n",
    "import PIL.Image as Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import pandas\n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "from models.siamese_resnet import SiameseResnet\n",
    "import log_compiler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "import random\n",
    "import PIL\n",
    "from tqdm.autonotebook import tqdm\n",
    "from torchvision.datasets import ImageFolder\n",
    "from mediumdataset import SiameseNetworkDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trfm_valid = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "trfm_train = transforms.Compose([\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "target_trfrm = transforms.Compose([\n",
    "    lambda x: [x],\n",
    "    torch.Tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters\n",
    "A few arbitrary predefined parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.003\n",
    "NUM_EPOCH = 10\n",
    "ARCHITECTURE = 'resnet101'\n",
    "PRINT_EVERY = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'dataset/train/facescrub'\n",
    "valid_path = 'dataset/valid/facescrub'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Siamese Network Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Siamese Pair Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating similar pair from\t dataset/train/facescrub: 100%|██████████| 98/98 [00:01<00:00, 73.34it/s]\n",
      "generating different pair from\t dataset/train/facescrub: 100%|██████████| 7679514/7679514 [00:53<00:00, 143549.85it/s]\n",
      "generating similar pair from\t dataset/valid/facescrub: 100%|██████████| 98/98 [00:00<00:00, 409.30it/s]\n",
      "generating different pair from\t dataset/valid/facescrub: 100%|██████████| 1356375/1356375 [00:07<00:00, 170782.11it/s]\n"
     ]
    }
   ],
   "source": [
    "trainset = SiamesePairDataset(root=train_path, ext='', transform=trfm_train, target_transform=target_trfrm, glob_pattern='*/*.[jJpP]*')\n",
    "validset = SiamesePairDataset(root=valid_path, ext='', transform=trfm_valid, target_transform=target_trfrm, glob_pattern='*/*.[jJpP]*')\n",
    "trainloader = data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validloader = data.DataLoader(validset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfolder = ImageFolder('dataset/train/facescrub')\n",
    "trainset = SiameseNetworkDataset(trainfolder, transform=trfm_valid, target_transform=target_trfrm, should_invert=False)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "validfolder = ImageFolder('dataset/valid/facescrub')\n",
    "validset = SiameseNetworkDataset(validfolder, transform=trfm_valid, target_transform=target_trfrm, should_invert=False)\n",
    "validloader = DataLoader(validset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train loader: 29, number of valid loader (13), each has 256\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of train loader: {len(trainloader)}, number of valid loader ({len(validloader)}), each has {BATCH_SIZE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive Loss\n",
    "This function used to calculate the loss/cost of our input image, based on this paper by Yan Lecun http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf. Because siamese is not classification problem rather a distance problem which means we need to compute the difference between two images hence we need another type of loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, device, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, output1, output2, Y):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        euclidean_distance = euclidean_distance.to(self.device)\n",
    "        loss_contrastive = torch.mean((1-Y) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (Y) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rounding(val, threshold=0.5):\n",
    "    return val > threshold\n",
    "\n",
    "def log_training_result(numepoch, batchsize, lrate, accuracy, precision, f1, tp, tn, fp, fn, fc, model, name='training_logs.csv'):\n",
    "    with open(name, 'r') as f:\n",
    "        train_number = len(f.readlines())\n",
    "    detail = log_compiler.compile(numepoch, batchsize, lrate, accuracy, precision, f1, tp, tn, fp, fn, fc, train_number, model)\n",
    "    data = [batchsize,lrate,numepoch,round(accuracy, 2),round(f1, 2),round(precision, 2),tp,tn,fp,fn,f'logs/{train_number}.md']\n",
    "    log_compiler.write_log_file(train_number, detail)\n",
    "    with open(name, 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(data)\n",
    "    print(\"data saved to %s\" % name)\n",
    "    \n",
    "def print_scores(acc, err, batch_size):\n",
    "    # just an utility printing function\n",
    "    for name, scores in zip((\"accuracy\", \"error\"), (acc, err)):\n",
    "        print(f\"\\t{name.rjust(14, ' ')}: {sum(scores)/batch_size:.4f}\")\n",
    "\n",
    "def confusion_matrix(y_pred, y_true, threshold=0.5):\n",
    "    y_pred, y_true = y_pred.view(-1), y_true.view(-1).int()\n",
    "    y_pred = rounding(y_pred, threshold).int()\n",
    "#     fn = (1-y_pred == y_true).sum()\n",
    "#     ap = (y_pred == y_true).sum()\n",
    "#     an = (y_pred != y_true).sum()\n",
    "#     fp = (y_pred == 1-y_true).sum()\n",
    "#     fn,fp,ap,an = fn.item(),fp.item(),ap.item(),an.item()\n",
    "#     error = (fn + fp) / (fp+fn+ap+an) * 100\n",
    "#     accuracy = (ap+an) / (fp+fn+ap+an) * 100\n",
    "    corrects = y_true == y_pred\n",
    "    accuracy = torch.mean(corrects.type(torch.FloatTensor))\n",
    "    return accuracy, 1-accuracy #accuracy, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "    \n",
    "def train(epoch, num_epoch, model, dataloader, criterion, optimizer):\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    for idx, ((img1,img2),label) in enumerate(dataloader):\n",
    "        data_time.update(time.time() - end_time)\n",
    "        img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output1, output2 = model.forward(img1, img2) #forward prop\n",
    "        loss = criterion(output1, output2, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.update(loss.item(), img1.size(0))\n",
    "        batch_time.update(time.time() - end_time)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        if idx % PRINT_EVERY == 0:\n",
    "            print(f'Train Epoch [{epoch+1}/{num_epoch}] [{idx}/{len(dataloader)}]\\t'\n",
    "                  f' Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  f' Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  f' Loss {losses.val:.4f} ({losses.avg:.4f}) ')\n",
    "        \n",
    "    return losses.avg\n",
    "        \n",
    "def valid(epoch, num_epoch, model, dataloader, criterion, optimizer):\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    accuracy = 0\n",
    "    accl = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        end_time = time.time()\n",
    "        for idx, ((img1,img2),label) in enumerate(dataloader):\n",
    "            data_time.update(time.time() - end_time)\n",
    "            img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "            output1, output2 = model.forward(img1, img2) #forward prop\n",
    "            loss = criterion(output1, output2, label)\n",
    "\n",
    "            predicted_label = F.pairwise_distance(output1, output2)\n",
    "            acc, err = confusion_matrix(predicted_label, label, threshold=0.5)\n",
    "            accuracy += acc\n",
    "            accl.append(acc)\n",
    "            \n",
    "            losses.update(loss.item(), img1.size(0))\n",
    "            batch_time.update(time.time() - end_time)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            if idx % PRINT_EVERY == 0:\n",
    "                print(f'Valid Epoch [{epoch + 1}/{num_epoch}] [{idx}/{len(dataloader)}]\\t'\n",
    "                      f' Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      f' Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                      f' Loss {losses.val:.4f} ({losses.avg:.4f})\\t Accuracy: {accuracy/5:.3f}')\n",
    "                accuracy = 0\n",
    "    print(f\"Valid accuracy one complete epoch: {sum(accl)/len(accl)}\")\n",
    "            \n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d34034ad6c4f8aba9dfa42e05cea14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg valid accuracy: 0.49775795294688296\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "progress = tqdm(enumerate(validloader, 1))\n",
    "len_valid = len(validloader)\n",
    "laccuracy = []\n",
    "with torch.no_grad():\n",
    "    for idx, ((img1,img2),label) in progress:\n",
    "        img1 = img1.to(device)\n",
    "        img2 = img2.to(device)\n",
    "        label = label.to(device)\n",
    "        out1, out2 = model.forward(img1,img2)\n",
    "        predicted_label = F.pairwise_distance(out1, out2)\n",
    "\n",
    "\n",
    "        acc, err = confusion_matrix(predicted_label, label, threshold=0.5)\n",
    "        progress.set_description(f'Val acc {idx}/{len_valid}: {acc.item():.3f}')\n",
    "        laccuracy.append(acc.item())\n",
    "    \n",
    "print(f\"Avg valid accuracy: {sum(laccuracy)/len(laccuracy)}\")\n",
    "# y_true = np.array(list(itertools.chain(*labels)))\n",
    "# y_pred = np.array(list(itertools.chain(*preds)))\n",
    "# tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred).ravel()\n",
    "# total = tn+tp+fp+fn\n",
    "# print(f'Predicted true and actually true: {tp}'\n",
    "#       f'\\nPredicted false and actually false: {tn}'\n",
    "#       f'\\nPredicted true but actually false: {fp}'\n",
    "#       f'\\nPredicted false but actually true: {fn}'\n",
    "#       f'\\nTotal correct predictions: {tp+tn} ({(tp+tn)/total*100:.2f})'\n",
    "#       f'\\nTotal wrong predictions: {fn+fp} ({(fn+fp)/total*100:.2f})'\n",
    "#       f'\\nTotal: ({total})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x,y,z = next(iter(valid_loader))\n",
    "# o1, o2 = model(x,y)\n",
    "# pred = F.pairwise_distance(o1,o2)\n",
    "\n",
    "# idx = 0\n",
    "# fig, ax = plt.subplots(ncols=2, nrows=1)\n",
    "# ax[0].imshow(x[idx].permute(1,2,0))\n",
    "# ax[1].imshow(y[idx].permute(1,2,0))\n",
    "# print(z[idx])\n",
    "# print(pred[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc = metrics.accuracy_score(y_true, y_pred)*100\n",
    "# f1 = metrics.f1_score(y_true, y_pred)*100\n",
    "# prec = metrics.precision_score(y_true, y_pred)*100\n",
    "# print(f'Batch Size: {BATCH_SIZE}\\t Learning Rate: {LEARNING_RATE}\\t NUM EPOCH: {NUM_EPOCH}')\n",
    "# print(f'Accuracy: {acc:.2f}\\t F1: {f1:.2f}\\t Precision: {prec:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for training_logs.csv purpose\n",
    "# log_training_result(NUM_EPOCH, BATCH_SIZE, LEARNING_RATE, acc, prec, f1, tp, tn, fp, fn, model.model.fc, ARCHITECTURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pandas.read_csv(\"training_logs.csv\")\n",
    "# df[-15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SiameseDensnet().to(device)\n",
    "\n",
    "# fc = nn.Sequential(\n",
    "#     nn.Linear(1024, 256),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(.2),\n",
    "    \n",
    "#     nn.Linear(256, 128),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(.2),\n",
    "    \n",
    "#     nn.Linear(128, 64),\n",
    "# )\n",
    "\n",
    "# model.set_classifier(fc)\n",
    "# model.freeze_all_except_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model created!\n"
     ]
    }
   ],
   "source": [
    "class SiameseTrainer(nn.Module):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, p, q):\n",
    "        return self.model(p), self.model(q)\n",
    "\n",
    "base = models.resnet101(pretrained=True)\n",
    "\n",
    "for param in base.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# for param in b\n",
    "    \n",
    "in_features = base.fc.in_features\n",
    "base.fc = nn.Sequential(\n",
    "    nn.Linear(in_features, 512),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(.2),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(.2),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(.2),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(64, 64)\n",
    ")\n",
    "\n",
    "criterion = ContrastiveLoss(device)\n",
    "optimizer = optim.SGD(base.fc.parameters(), lr=.0035)\n",
    "\n",
    "base.to(device)\n",
    "base = nn.DataParallel(base)\n",
    "model = SiameseTrainer(base)\n",
    "\n",
    "print('model created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(base.module.fc.parameters(), lr=.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltrain, lvalid = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch [1/10] [0/29]\t Time 20.950 (20.950)\t Data 15.619 (15.619)\t Loss 3.4570 (3.4570) \n",
      "Valid Epoch [1/10] [0/13]\t Time 21.398 (21.398)\t Data 18.509 (18.509)\t Loss 1.1113 (1.1113)\t Accuracy: 0.096\n",
      "Valid accuracy one complete epoch: 0.49123984575271606\n",
      "Train Epoch [2/10] [0/29]\t Time 20.942 (20.942)\t Data 17.886 (17.886)\t Loss 2.4643 (2.4643) \n",
      "Valid accuracy one complete epoch: 0.4890440106391907\n",
      "Train Epoch [4/10] [0/29]\t Time 21.474 (21.474)\t Data 18.414 (18.414)\t Loss 1.5173 (1.5173) \n",
      "Valid Epoch [4/10] [0/13]\t Time 22.859 (22.859)\t Data 19.891 (19.891)\t Loss 1.0508 (1.0508)\t Accuracy: 0.102\n",
      "Valid accuracy one complete epoch: 0.5182137489318848\n",
      "Train Epoch [5/10] [0/29]\t Time 24.229 (24.229)\t Data 21.074 (21.074)\t Loss 1.3452 (1.3452) \n",
      "Valid Epoch [5/10] [0/13]\t Time 24.699 (24.699)\t Data 21.804 (21.804)\t Loss 1.0562 (1.0562)\t Accuracy: 0.104\n",
      "Valid accuracy one complete epoch: 0.5117880702018738\n",
      "Train Epoch [6/10] [0/29]\t Time 25.058 (25.058)\t Data 21.976 (21.976)\t Loss 1.2413 (1.2413) \n",
      "Valid Epoch [6/10] [0/13]\t Time 22.091 (22.091)\t Data 19.157 (19.157)\t Loss 1.0595 (1.0595)\t Accuracy: 0.101\n",
      "Valid accuracy one complete epoch: 0.49754995107650757\n",
      "Train Epoch [7/10] [0/29]\t Time 28.372 (28.372)\t Data 25.238 (25.238)\t Loss 1.1577 (1.1577) \n",
      "Valid Epoch [7/10] [0/13]\t Time 22.951 (22.951)\t Data 20.047 (20.047)\t Loss 1.0480 (1.0480)\t Accuracy: 0.096\n",
      "Valid accuracy one complete epoch: 0.4820173680782318\n",
      "Train Epoch [8/10] [0/29]\t Time 22.796 (22.796)\t Data 19.723 (19.723)\t Loss 1.1065 (1.1065) \n",
      "Valid Epoch [8/10] [0/13]\t Time 23.122 (23.122)\t Data 20.172 (20.172)\t Loss 1.0608 (1.0608)\t Accuracy: 0.098\n",
      "Valid accuracy one complete epoch: 0.4978504180908203\n",
      "Train Epoch [9/10] [0/29]\t Time 25.171 (25.171)\t Data 22.040 (22.040)\t Loss 1.0418 (1.0418) \n",
      "Valid Epoch [9/10] [0/13]\t Time 22.880 (22.880)\t Data 19.981 (19.981)\t Loss 1.0358 (1.0358)\t Accuracy: 0.090\n",
      "Valid accuracy one complete epoch: 0.47429734468460083\n",
      "Train Epoch [10/10] [0/29]\t Time 21.852 (21.852)\t Data 18.793 (18.793)\t Loss 1.0747 (1.0747) \n",
      "Valid Epoch [10/10] [0/13]\t Time 19.369 (19.369)\t Data 16.492 (16.492)\t Loss 1.0417 (1.0417)\t Accuracy: 0.085\n",
      "Valid accuracy one complete epoch: 0.45795580744743347\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c9cb2c4ed29a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCH):\n",
    "    trainloss = train(epoch, NUM_EPOCH, model, trainloader, criterion, optimizer)\n",
    "    validloss = valid(epoch, NUM_EPOCH, model, validloader, criterion, optimizer)\n",
    "    ltrain.append(trainloss)\n",
    "    lvalid.append(validloss)\n",
    "\n",
    "plt.plot(ltrain, label='train_loss')\n",
    "plt.plot(lvalid, label='valid_loss')\n",
    "plt.legend(frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8VPW9//HXZ7IvhCQQwhoSWUWBAJEtKC7VIlWpSi3UDZRyqbhgq9X20dvFtvfnvXrVq1WsVbBaFBVBUam4i4CAAdn3JSHsgbBkIcskn98fM0CALBOY5Ewyn+fjMY85c873zHwyyvuc8z1nvkdUFWOMMcHD5XQBxhhjGpcFvzHGBBkLfmOMCTIW/MYYE2Qs+I0xJshY8BtjTJCx4DfGmCBjwW+MMUHGgt8YY4JMqNMFVKd169aamprqdBnGGNNkLF++/KCqJvnSNiCDPzU1laysLKfLMMaYJkNEcnxta109xhgTZCz4jTEmyFjwG2NMkLHgN8aYIGPBb4wxQcaC3xhjgowFvzHGBJlmE/wl5RX8Y8F2vt12yOlSjDEmoAXkD7jOhUuEf3yznR5tWzCkSyunyzHGmIDVbPb4w0Nd3Dk0lW+2HGTz/gKnyzHGBJAjR47wwgsv1Hu9kSNHcuTIkXqvN27cOGbNmlXv9RpLswl+gLEDU4gIdTF90Q6nSzHGBJCagr+ioqLW9ebNm0d8fHxDleWYZtPVA5AYE85N/Tswe8VuHv5hTxJjwp0uyRhzhj99sI71e4759T17tY/jD9dfVOPyRx99lG3btpGenk5YWBixsbG0a9eOlStXsn79en784x+Tm5tLSUkJDzzwABMnTgROjRtWWFjItddey7Bhw1i8eDEdOnTg/fffJyoqqs7aPv/8cx566CHcbjeXXHIJU6dOJSIigkcffZS5c+cSGhrKNddcw5NPPsk777zDn/70J0JCQmjZsiULFizw23dUVbPa4wcYn5lGqbuSN5b6PF6RMaaZe/zxx+nSpQsrV67kiSeeYNmyZfz1r39l/fr1AEybNo3ly5eTlZXFs88+y6FDZ18ksmXLFiZPnsy6deuIj4/n3XffrfNzS0pKGDduHG+99RZr1qzB7XYzdepU8vPzmTNnDuvWrWP16tX87ne/A+Cxxx5j/vz5rFq1irlz5/r3S6iiWe3xA3RPbsGl3Vrz2rc5TLysC+GhzW7bZkyTVtueeWMZOHAgaWlpJ18/++yzzJkzB4Dc3Fy2bNlCq1anXySSlpZGeno6AAMGDCA7O7vOz9m0aRNpaWl0794dgDvvvJPnn3+ee++9l8jISCZMmMCPfvQjrrvuOgAyMzMZN24ct9xyCzfddJM//tRq1ZmKItJJRL4UkQ0isk5EHqimzcMistL7WCsiFSKS6F2WLSJrvMsaZazlu4alcaCglHlr9jbGxxljmpiYmJiT01999RWfffYZ3377LatWraJfv36UlJSctU5ERMTJ6ZCQENxud52fo6rVzg8NDWXZsmXcfPPNvPfee4wYMQKAF198kb/85S/k5uaSnp5e7ZGHP/iyx+8GfqWqK0SkBbBcRD5V1fUnGqjqE8ATACJyPfCgquZXeY8rVPWgPwuvzfBuSVyQFMO0RTsYld4eEWmsjzbGBKAWLVpQUFD91X5Hjx4lISGB6OhoNm7cyJIlS/z2uT179iQ7O5utW7fStWtXXn/9dYYPH05hYSHFxcWMHDmSwYMH07VrVwC2bdvGoEGDGDRoEB988AG5ublnHXn4Q53Br6p7gb3e6QIR2QB0ANbXsMpY4E2/VXgOXC7hrsw0fvfeWrJyDnNJaqKT5RhjHNaqVSsyMzO5+OKLiYqKIjk5+eSyESNG8OKLL9KnTx969OjB4MGD/fa5kZGRTJ8+nZ/85CcnT+5OmjSJ/Px8Ro0aRUlJCarK008/DcDDDz/Mli1bUFWuuuoq+vbt67daqpKaDkWqbSySCiwALlbVs07Li0g0sAvoemKPX0R2AIcBBf6uqi/V9TkZGRl6vnfgKi5zM+T/fcHQLq2YetuA83ovY4wJdCKyXFUzfGnr85lPEYkF3gWmVBf6XtcDi87o5slU1f7AtcBkEbmshvefKCJZIpKVl5fna1k1ig4PZezAFOav20dufvF5v58xxjQXPgW/iIThCf0Zqjq7lqZjOKObR1X3eJ8PAHOAgdWtqKovqWqGqmYkJfl0v+A63TGkMyLCPxdn++X9jDGmqsmTJ5Oenn7aY/r06U6XVac6+/jFc2b0FWCDqj5VS7uWwHDgtirzYgCX99xADHAN8Nh5V+2j9vFRjOzdjre+y2XK1d2JjWh2V68aYxz0/PPPO13COfFljz8TuB24ssolmyNFZJKITKrS7kbgE1UtqjIvGVgoIquAZcBHqvqx36r3wV2ZqRSUupmVlduYH2uMMQHLl6t6FgJ1Xg+pqq8Cr54xbzvQMKelfdQvJYF+KfFMX5zNHUNScbns0k5jTHALip+13j0sjZxDxXy+8YDTpRhjjOOCIvhHXNSW9i0jmbbQRu00xpigCP7QEBd3DE3l2+2H/D4qoDGm+YmNjQVgz549jB49uto2l19+ObX93ig1NZWDBxttwIJ6CYrgBxh7SQpRYSFMs7H6jTE+at++fUDfUOVcBc31jS2jwxg9oCNvfZfLIyN6ktQiou6VjDH+9+9HYd8a/75n295w7eM1Ln7kkUfo3Lkz99xzDwB//OMfEREWLFjA4cOHKS8v5y9/+QujRo06bb3s7Gyuu+461q5dy/Hjxxk/fjzr16/nwgsv5Pjx4z6X99RTTzFt2jQAJkyYwJQpUygqKuKWW25h165dVFRU8J//+Z/89Kc/rXacfn8LmuAHGJeZyutLcpixNIcpP+judDnGmEYyZswYpkyZcjL43377bT7++GMefPBB4uLiOHjwIIMHD+aGG26ocVDHqVOnEh0dzerVq1m9ejX9+/f36bOXL1/O9OnTWbp0KarKoEGDGD58ONu3b6d9+/Z89NFHgGewuBPj9G/cuBEROafbPvoiqIK/S1IsV/RI4l9LcvjF5V2ICA1xuiRjgk8te+YNpV+/fhw4cIA9e/aQl5dHQkIC7dq148EHH2TBggW4XC52797N/v37adu2bbXvsWDBAu6//34A+vTpQ58+fXz67IULF3LjjTeeHAr6pptu4ptvvmHEiBE89NBDPPLII1x33XVceumluN3uasfp97eg6eM/4e5hF3CwsIy5K/c4XYoxphGNHj2aWbNm8dZbbzFmzBhmzJhBXl4ey5cvZ+XKlSQnJ1c7Dn9V5zLEe00DYXbv3p3ly5fTu3dvfvOb3/DYY4/VOE6/vwVd8Gd2bUX35FimLcqu8T+IMab5GTNmDDNnzmTWrFmMHj2ao0eP0qZNG8LCwvjyyy/Jyan9dq2XXXYZM2bMAGDt2rWsXr3ap8+97LLLeO+99yguLqaoqIg5c+Zw6aWXsmfPHqKjo7ntttt46KGHWLFiBYWFhRw9epSRI0fyzDPPsHLlyvP+u6sTVF094Nli35WZxqOz17Bkez5Duvj/JgfGmMBz0UUXUVBQQIcOHWjXrh233nor119/PRkZGaSnp9OzZ89a1//FL37B+PHj6dOnD+np6QwcWO14k2fp378/48aNO9l+woQJ9OvXj/nz5/Pwww/jcrkICwtj6tSpFBQUVDtOv7/Vazz+xuKP8fhrU1JewdDHv2BA5wT+cYdPw1cbY0xAa5Dx+JuTyLAQbh2Uwmcb9pN9sKjuFYwxphkJyuAHuG1wZ0Jdwqs2Vr8x5jwMGjTorDH516zx8+8U/Czo+vhPSI6L5Lo+7XknK5dfXtOduMgwp0syxjRBS5cudbqEegvaPX6AuzLTKCqr4O3vbKx+Y0zwCOrg792xJQNTE5m+KBt3RaXT5RhjTKMI6uAHuGtYKruPHOezDfudLsUYYxpF0Af/1b3a0jEhimkLs50uxRhjGkXQB3+ISxg3NJVl2fms2XXU6XKMMabB1Rn8ItJJRL4UkQ0isk5EHqimzeUicrTKzdh/X2XZCBHZJCJbReRRf/8B/nDLJZ2ICbex+o0xwcGXPX438CtVvRAYDEwWkV7VtPtGVdO9j8cARCQEeB64FugFjK1hXUfFRYbxk4xOfLh6D/uP1T5IkzHGNHV1Br+q7lXVFd7pAmAD0MHH9x8IbFXV7apaBswERtWxjiPGZ6birlRe/7b2gZqMMaapq1cfv4ikAv2A6n6xMEREVonIv0XkIu+8DkDVi+R3UcNGQ0QmikiWiGTl5eXVpyy/6Nwqhh9cmMyMpTmUlFc0+ucbY0xj8Tn4RSQWeBeYoqpn3rF8BdBZVfsCzwHvnVitmreqdlQ4VX1JVTNUNSMpKcnXsvzqrsw0DheX8973ux35fGOMaQw+Bb+IhOEJ/RmqOvvM5ap6TFULvdPzgDARaY1nD79TlaYdgYC9A8rgCxK5sF0c0xbtsLH6jTHNli9X9QjwCrBBVZ+qoU1bbztEZKD3fQ8B3wHdRCRNRMKBMcBcfxXvbyLC3cPS2Ly/kIVbDzpdjjHGNAhf9vgzgduBK6tcrjlSRCaJyCRvm9HAWhFZBTwLjFEPN3AvMB/PSeG3VXVdA/wdfnN933a0jg1n2kK7tNMY0zzVOTqnqi6k+r76qm3+BvythmXzgHnnVJ0DIkJDuG1wZ575bAvb8grpkhTrdEnGGONXQf/L3ercNrgz4SEuXl2U7XQpxhjjdxb81WgdG8Go9PbMWr6LI8VlTpdjjDF+ZcFfg/GZaRwvr2CmjdVvjGlmLPhr0Kt9HEMuaMU/F2dTbmP1G2OaEQv+Wtw9LI29R0v4eO0+p0sxxhi/seCvxZU929C5VbSN2mmMaVYs+Gvhcgnjh6by/c4jrNh52OlyjDHGLyz46/CTjE60iAxlul3aaYxpJiz46xATEcqYSzoxb81e9hw57nQ5xhhz3iz4fXDHkFRUlddsrH5jTDNgwe+DTonR/PCitry5bCfFZW6nyzHGmPNiwe+ju4elcfR4ObNX2Fj9xpimzYLfRwM6J9CnY0umLdpBZaWN1W+Mabos+H0kItyVmcb2vCK+3tL4t4Y0xhh/seCvh5G925EcF2Fj9RtjmjQL/noID3Vxx5BUvtlykM37C5wuxxhjzokFfz2NHZhCRKiL6TaMgzGmibLgr6fEmHBu6t+B2St2k19kY/UbY5oeX2623klEvhSRDSKyTkQeqKbNrSKy2vtYLCJ9qyzLFpE13nv1Zvn7D3DCXZlplLoreXPZTqdLMcaYevNlj98N/EpVLwQGA5NFpNcZbXYAw1W1D/Bn4KUzll+hqumqmnHeFQeAbsktuLRba/65OJsyt43Vb4xpWuoMflXdq6orvNMFwAagwxltFqvqieErlwAd/V1ooLlrWBoHCkqZt2av06UYY0y91KuPX0RSgX7A0lqa3Q38u8prBT4RkeUiMrG+BQaq4d2S6JIUw7RFO1C1H3QZY5oOn4NfRGKBd4EpqnqshjZX4An+R6rMzlTV/sC1eLqJLqth3YkikiUiWXl5gf8DKZdLGJ+ZxupdR8nKsbH6jTFNh0/BLyJheEJ/hqrOrqFNH+BlYJSqHjoxX1X3eJ8PAHOAgdWtr6ovqWqGqmYkJSXV769wyE39O9AyKsx+0GWMaVJ8uapHgFeADar6VA1tUoDZwO2qurnK/BgRaXFiGrgGWOuPwgNBdHgoYwemMH/dPnLzi50uxxhjfOLLHn8mcDtwpfeSzJUiMlJEJonIJG+b3wOtgBfOuGwzGVgoIquAZcBHqvqxv/8IJ905tDMiwmvfZjtdijHG+CS0rgaquhCQOtpMACZUM3870PfsNZqPdi2jGNm7HTOX5fLAD7oTG1HnV2qMMY6yX+76wV2ZqRSUupmVlet0KcYYUycLfj/ol5JA/5R4pi/OtrH6jTEBz4LfT+4alkbOoWK+2HjA6VKMMaZWFvx+MuKitrRvGckrdmmnMSbAWfD7SWiIizuGpvLt9kOs31Pt79uMMSYgWPD70dhLUogKC7Gx+o0xAc2C349aRocxekBH3l+5h7yCUqfLMcaYalnw+9n4zFQqVHnsw/U2eJsxJiBZ8PvZBUmx/PLq7nywag/vZO1yuhxjjDmLBX8DmDS8C0O7tOIPc9ex9YDdlN0YE1gs+BtAiEt4+qfpRIWHcO8b31NSXuF0ScYYc5IFfwNJjovkf3/Sl437CviveRucLscYY06y4G9AV/Rsw4Rhabz2bQ7z1+1zuhxjjAEs+Bvcr0f0pHeHlvx61mp2HznudDnGGGPB39DCQ108N7Yf7opKpsz8HndFpdMlGWOCnAV/I0htHcNfb+zNd9mHefaLrU6XY4wJchb8jeTH/Tpwc/+OPPfFFr7ddqjuFYwxpoFY8Deix0ZdRFqrGKa89T35RWVOl2OMCVIW/I0oJiKU537Wj8NF5Tz8ziob0sEY44g6g19EOonIlyKyQUTWicgD1bQREXlWRLaKyGoR6V9l2Z0issX7uNPff0BTc1H7lvxmZE8+33iA6YuynS7HGBOEfNnjdwO/UtULgcHAZBHpdUaba4Fu3sdEYCqAiCQCfwAGAQOBP4hIgp9qb7LGDU3lBxe24fF/b2Tt7qNOl2OMCTJ1Br+q7lXVFd7pAmAD0OGMZqOA19RjCRAvIu2AHwKfqmq+qh4GPgVG+PUvaIJEhCdG9yUxJpz73vyewlK30yUZY4JIvfr4RSQV6AcsPWNRByC3yutd3nk1zQ96CTHhPDMmnZxDRfz+/bVOl2OMCSI+B7+IxALvAlNU9cx7C0o1q2gt86t7/4kikiUiWXl5eb6W1aQNvqAV913ZjdkrdjPnexvC2RjTOHwKfhEJwxP6M1R1djVNdgGdqrzuCOypZf5ZVPUlVc1Q1YykpCRfymoW7ruyKwNTE/ndnLXsOFjkdDnGmCDgy1U9ArwCbFDVp2poNhe4w3t1z2DgqKruBeYD14hIgvek7jXeecYrNMTFM2PSCQ1xcd+bKyh12xDOxpiG5csefyZwO3CliKz0PkaKyCQRmeRtMw/YDmwF/gHcA6Cq+cCfge+8j8e880wV7eOjeGJ0H9buPsb/fLzJ6XKMMc1caF0NVHUh1ffVV22jwOQalk0Dpp1TdUHkmovacueQzryycAeZXVtxZc9kp0syxjRT9svdAPKbkRdyYbs4HnpnNfuPlThdjjGmmbLgDyCRYSE8N7Yfx8sqmDJzJRWVNqSDMcb/LPgDTNc2sfxp1EV8u/0QL3xpQzgbY/zPgj8A/WRAR27o255nPt9CVradCzfG+JcFfwASEf5648V0iI/igZkrOVJsQzgbY/zHgj9AtYgM47mx/dh/rIRH3l1tQzgbY/zGgj+A9e0Uz69H9GD+uv38a+lOp8sxxjQTFvwBbsKwCxjePYk/f7ieDXvPHCLJGGPqz4I/wLlcwv/e0peWUWHc9+b3FJfZEM7GmPNjwd8EtI6N4Olb0tmWV8hjH6x3uhxjTBNnwd9EDOvWml8M78LM73L5YFW1A5waY4xPLPibkAev7k6/lHh+O3sNufnFTpdjjGmiLPibkLAQF8+O6QcC9735PeUVlU6XZIxpgiz4m5hOidE8flMfVuYe4X8/2ex0OcaYJsiCvwn6UZ92jB2Ywotfb2PB5uC4TaUxxn8s+Juo31/Xi+7Jsfzy7VXkFZQ6XY4xpgmx4G+iosJDeG5sfwpKyvnl2yuptCGcjTE+suBvwnq0bcHvr+/FN1sO8tI3250uxxjTRFjwN3E/G5jCtRe35cn5m/h+52GnyzHGNAF1Br+ITBORAyKytoblD1e5CftaEakQkUTvsmwRWeNdluXv4o1nCOfHb+pDclwk98/8nmMl5U6XZIwJcL7s8b8KjKhpoao+oarpqpoO/Ab4WlWr3j3kCu/yjPMr1dSkZXQYz47tx54jJfx29hobwtkYU6s6g19VFwC+3gZqLPDmeVVkzsmAzgn88urufLh6L29n5TpdjjEmgPmtj19EovEcGbxbZbYCn4jIchGZWMf6E0UkS0Sy8vLs2vRzMWl4FzK7tuIPc9ex9UCB0+UYYwKUP0/uXg8sOqObJ1NV+wPXApNF5LKaVlbVl1Q1Q1UzkpKS/FhW8AhxCU/fkk5MeCj3vvE9JeUVTpdkjAlA/gz+MZzRzaOqe7zPB4A5wEA/fp6pRpu4SJ68pS8b9xXw1482OF2OMSYA+SX4RaQlMBx4v8q8GBFpcWIauAao9sog419X9GjDzy9N4/UlOfz3xxvtx13GmNOE1tVARN4ELgdai8gu4A9AGICqvuhtdiPwiaoWVVk1GZgjIic+5w1V/dh/pZva/HpET4rKKpj61Ta27C/gmTH9iI2o8z+3MSYISCBe+peRkaFZWXbZ//lSVV5fksOfPlhPl6QYXr7jElJaRTtdljGmAYjIcl8vm7df7jZjIsIdQ1L55/iB7DtawqjnF7Jk+yGnyzLGOMyCPwgM69aa9+8dRkJMOLe9vJQ3lu50uiRjjIMs+INEWusY5tyTSWbX1vx2zhr+OHcdbruDlzFByYI/iLSMCmPauEuYMCyNVxdnM276dxwttrF9jAk2FvxBJsQl/O66XvzP6D4s3XGIH7+wiK0HCp0uyxjTiCz4g9QtGZ144+eDOXa8nBtfWMTXdgtHY4KGBX8QuyQ1kffvzaRjQjTjpy/jlYU7bGRPY4KABX+Q65gQzaxJQ7i6VzJ//nA9j7y7mlK3jfFjTHNmwW+IiQhl6q0DuP/KrrydtYvbXl7KwUK7gbsxzZUFvwHA5RJ+eU0Pnhvbj9W7jjLqb4vYsPeY02UZYxqABb85zfV92/POpCG4Kyu5eepi5q/b53RJxhg/s+A3Z+nTMZ659w6jW3IL/uP15fztiy120teYZsSC31QrOS6StyYO5sfp7Xnyk83cP3Ol3djFmGbCxuk1NYoMC+Hpn6bTvW0Lnpi/iZxDRbx0ewZtW0Y6XZox5jzYHr+plYhwz+Vdeen2DLYdKOSGvy1kZe4Rp8syxpwHC37jk6t7JfPuPUMJD3Vxy9+/5f2Vu50uyRhzjiz4jc96to3j/cmZpHeK54GZK/kfu62jMU2SBb+pl1axEfzr7kGMHdiJF77axn/8azmFpW6nyzLG1EOdwS8i00TkgIhUe6N0EblcRI6KyErv4/dVlo0QkU0islVEHvVn4cY54aEu/uvG3vzx+l58vmE/o6cuJje/2OmyjDE+8mWP/1VgRB1tvlHVdO/jMQARCQGeB64FegFjRaTX+RRrAoeIMC4zjX/eNZA9R44z6vlFLLXbOhrTJNQZ/Kq6AMg/h/ceCGxV1e2qWgbMBEadw/uYAHZptyTem5xJfFQYt72ylJnL7LaOxgQ6f/XxDxGRVSLybxG5yDuvA5Bbpc0u7zzTzFyQFMucyZkM6dKaR2ev4U8f2G0djQlk/gj+FUBnVe0LPAe8550v1bSt8RIQEZkoIlkikpWXZzcFaWpaRoUx7c4M7spMY/qibMa/ard1NCZQnXfwq+oxVS30Ts8DwkSkNZ49/E5VmnYE9tTyPi+paoaqZiQlJZ1vWcYBoSEufn99L/775t4s2X6IG19YxPY8u62jMYHmvINfRNqKiHinB3rf8xDwHdBNRNJEJBwYA8w9388zge+nl6QwY8JgjhwvZ9TzdltHYwKNL5dzvgl8C/QQkV0icreITBKRSd4mo4G1IrIKeBYYox5u4F5gPrABeFtV1zXMn2ECzcC0RN6fnEmH+CjunLaMn7+WxcZ9Nr6/MYFAAnG43YyMDM3KynK6DOMHxWVuXvlmBy99s53CUjc39G3Pgz/oTmrrGKdLM6ZZEZHlqprhU1sLftMYjhSX8fcF23l1UTZlFZXcktGR+67sRvv4KKdLM6ZZsOA3AetAQQkvfLmNN5buBIHbBnXmniu60Do2wunSjGnSLPhNwNt1uJhnP9/CrOW7iAwLYXxmKhMv7ULL6DCnSzOmSbLgN03GtrxCnv50Mx+u3ktcZCj/MbwL44amEhNh9wgypj4s+E2Ts37PMZ76dBOfbThA69hw7rm8Kz8blEJkWIjTpRnTJFjwmyZrxc7DPDl/E4u3HaJ9y0juv6obowd0JDTERhA3pjb1CX7712QCSv+UBN74+WBmTBhEm7hIHp29hqufXsD7K3fbTV+M8RMLfhOQMru2Zs49Q3n5jgwiQl08MHMlI5/9hk/X7ycQj1KNaUos+E3AEhF+0CuZefdfyrNj+1HqruTnr2Vx4wuLWbT1oNPlGdNkWfCbgOdyCTf0bc+nD17Gf9/cmwPHSrj15aWMfWkJy3MOO12eMU2Ondw1TU5JeQVvLtvJ819u5WBhGVf1bMOvrulBr/ZxTpdmjGPsqh4TFIpK3by6OJu/f72NYyVuruvTjgev7k6XpFinSzOm0Vnwm6By9Hg5/1iwnWmLdlBSXsHoAR25/6pudEyIdro0YxqNBb8JSgcLS3nhy238a0kOAD8blMI9V3ShTYtIhyszpuFZ8JugtufIcZ77YgtvZ+0iPMTFnUNTmTT8AuKjw50uzZgGY8FvDLDjYBHPfLaZuav2EBseys8GpXDVhcn0T4m3XwKbZseC35gqNu47xlOfbObzjQeoqFRaRIZyabfWXN69DcN7JJEcZ11Bpumz4DemGsdKylm05SBfbcrjq80H2H+sFIBe7eK4vEcSl/doY0cDpsmy4DemDqrKhr0FfLX5AF9tymN5zuHTjwZ6tOHy7km0saMB00T4NfhFZBpwHXBAVS+uZvmtwCPel4XAL1R1lXdZNlAAVABuX4uy4DeN7cTRwJebDvD15jw7GjBNjr+D/zI8gf5aDcE/FNigqodF5Frgj6o6yLssG8hQ1XoNrGLBb5xU09FAXGQol3ZLYniPJDsaMAGnPsFf522OVHWBiKTWsnxxlZdLgI6+fLAxgUpE6NU+jl7t47jn8q4cPV7Ooq0H+WqTZ0Pw0Zq9wKmjgSt6tqFfJzsaME2Hv+9vdzfw7yqvFfhERBT4u6q+5OfPM6bBtYwKY2Tvdozs3e70o4GNefx9wXZe+GqbHQ2YJsVvwS8iV+AJ/mFVZmeq6h4RaQN8KiIbVXVBDetPBCYCpKSk+KssY/yqPkcDV/T0nBuwowETaHy6qsfb1fNDFe/pAAALpUlEQVRhdX383uV9gDnAtaq6uYY2fwQKVfXJuj7P+vhNU3TiaODLTQf4elMey3fauQHTePzax+/Dh6UAs4Hbq4a+iMQALlUt8E5fAzx2vp9nTKCqejQw+Yqajwa6J8cyoHMiGZ0TyEhNICUxGhFxuHoTTHy5qudN4HKgNbAf+AMQBqCqL4rIy8DNQI53FbeqZojIBXiOAsCzgXlDVf/qS1G2x2+aG1Vl/d5jfLUpj2U78lmx8zAFJW4AklpEkNE5gQGdE8hITeSi9nGEWdeQqSf7AZcxAa6yUtl8oICs7MNkZeeTlXOYXYePAxAZ5iK9UzwZnRMZkJpA/5QEWkaFOVyxCXQW/MY0QfuPlXg2BDn5ZGUfZv3eY1RUKiLQvU0LMlI9XUMZnRPpmBBl3UPmNBb8xjQDRaVuVuUeISvnMFk5h1mRc5jCUk/3UJsWESc3AhmpCVzYzrqHgl2jntw1xjSMmIhQhnZtzdCurQGoqFQ27StgeY6naygr+zDz1uwDICosxNM9lOo5T9AvJZ64SOseMtWzPX5jmrC9R4+TlX2Y5TmeLqL1e45RqSACPZJbnHZU0CHeuoeaM+vqMSZIFZa6WbnzCFk5+Sz3dg8VlVUA0DYukgGpCZ7LSDsn0i05lsiwEIcrNv5iXT3GBKnYiFCGdWvNsG6e7iF3RSUb9xV4jwg8VxB9tHrvyfbJcRGkJEbTKTGalMRoOreKPvk6KTbCjhCaKdvjNybI7D5ynBU5h8k+WEROfjE784vJzS9m79GS09pFhYWc3Aic2CCkJEaT0iqajglRRITa0UIgsT1+Y0yNOsRH0SE+6qz5JeUV7Dp8nFzvxiDn0KmNwqKtBzleXnGyrYin6+jkxsC7QTgxnRgTbkcLAcyC3xgDQGRYCF3bxNK1TexZy1SVvMLSajcKX2/O40BB6WntYyNCvd1HUXRuFXOqKykxmvbxUYSH2qWnTrLgN8bUSURo0yKSNi0iGdA58azlx8sq2HX47I3C9rwivtqUR6m78mRbl0C7llF0bhVNp4RoWrcIJzEmglYx4SR6H61iPc/WndQwmlfwL/o/qCh3ugrPcXB4C4iKh8h4iGx5ajoqHkIjnK7QGL+KCg+hW3ILuiW3OGtZZaXnaOHMjcLO/GK+3HSA/KIy3JXVn2uMjQg9tTE4sWGIDad1TMTJ6VYnl0cQFW4bCl80r+D/6nEoL3a6irqFRp2+IajxueXZ88LO7ps1JpC5XEJyXCTJcZFcknr20YKqcuy4m0NFpRwqKuNQYRn5RWXke1/nex97j5awbs8x8ovKKKuorOaTPCekqx4xnNpgRJy24TgxHRsRGpTnIppX8D+SDQTAf0SthLJCOH4ESo5UeT58xusjUHIUju6C/Ws9r8sKan/vkIi6NxonjjCiEiAqEaITPdMh9ktO4yBVz//3x/Z4H7uhMA/RSlqKi5YiXCAuOPGIdEGUC5JOzBMQFyouSiuUwrJKCssqKSqtpKDUM32stIKC0gqOlVZScKiCY3sq2VXiZlsFVCIoLioR78NFSIiL2IhwYqPCiQwPIzI8jKjwMCLCw4iKCCcqPIzIiDCiI8KIjggnKiKC6IhQYiIjiI4MJyYynKjwcFwhIafqdoWAhFSZ9tYeQJpX8AdSF0pYJMS0rv96FW4oPVbDRqKa54K9kLfBswEpOYbnbpc1iGgJ0Sc2Bq28GwTvhuG06Vanpu0Iw/iishKK8jxhXrD3VLCfDHnvw338vD9KgEjvw6d/YSHeR03cQB37W+dLEVRCPM9VNgzicnmfQxFXCBKbBJMWNmwxNLfgbw5CQk8FcX1VVng3GlWOMIrzqzznQ/Ehz3TxQTi42TNd21FGaJR3I1HdBqPqdMKpDUZEi4DbwzHnoaLcG+Z7T4V5wd7Tg71gL1S6T1/PFQot2kNce2jXB3pc65mOa39qfmyyp51W1vFQP7SpbXnFqTaVFSdfl7vdlJaVU1pWTklZOaXlnunScs/8svJyysrdlLnLKS9zU17hprzcTbnbjdv7KHe7qahwIyghVOKiEpd3+sS8EO8xCMWx/KwR/pNa8DcnrhBv905C/dZzl3k3Doe8G4czNhInNhzFh2Df6lPzajq6cIWdfgQREec95JVTh8PVPs5cHlLHcl/e44xlqp666/3MOa5X9dlLXJ5uN1eY5zkkDELCPQF4crq2ZaGe55Cw06fPWubDic6y4rND/GSYe58LD5z93zos2hvg7aBz5qlAP/noANGtweXrZZuBeXlnmPdx9gWu9VNZqRSXV1BU6qagxE1hqZvCEjeFpeWnvXa5GmeHyYLfQGg4tEj2PHxVWeHpXjprI5F/aiNxPB+KD3tCpdq9rQrOeW+tssq6tXVvNQrxHuH4+FxZARVlnr+/oes6uVE4Y0PjCoWig54jwzNFxp8K8La9T+2dx3XwPrfztLGjOp+5XEJsRCixEaEkxzldjQW/OVeukHPvkvI3Vd+7Ak6EVX2Cuqbn8w2+ykpP90hFGVSWe7pUKsq9r73zK8pPn64o97Y98/WZy6p53zOno1udCvQW7bzB3g7CY87v7zIBz6fgF5FpwHXAAVW9uJrlAvwfMBIoBsap6grvsjuB33mb/kVV/+mPwo056WQIB2Z3QY1cLnCFe464jGlEvv5LeRUYUcvya4Fu3sdEYCqAiCTiuTn7IGAg8AcRqWcHtDHGGH/yKfhVdQGQX0uTUcBr6rEEiBeRdsAPgU9VNV9VDwOfUvsGxBhjTAPz17FxByC3yutd3nk1zTfGGOMQfwV/dWe5tJb5Z7+ByEQRyRKRrLy8PD+VZYwx5kz+Cv5dQKcqrzsCe2qZfxZVfUlVM1Q1IykpyU9lGWOMOZO/gn8ucId4DAaOqupeYD5wjYgkeE/qXuOdZ4wxxiG+Xs75JnA50FpEduG5UicMQFVfBObhuZRzK57LOcd7l+WLyJ+B77xv9Ziq1naS2BhjTAPzKfhVdWwdyxWYXMOyacC0+pdmjDGmIQTkzdZFJA/IOcfVWwMH/VhOU2bfxens+zidfR+nNIfvorOq+nSCNCCD/3yISJavd5pv7uy7OJ19H6ez7+OUYPsumthv3I0xxpwvC35jjAkyzTH4X3K6gABi38Xp7Ps4nX0fpwTVd9Hs+viNMcbUrjnu8RtjjKlFswl+ERkhIptEZKuIPOp0PU4SkU4i8qWIbBCRdSLygNM1OU1EQkTkexH50OlanCYi8SIyS0Q2ev8fGeJ0TU4SkQe9/07WisibIhLpdE0NrVkEv4iEAM/juS9AL2CsiPRytipHuYFfqeqFwGBgcpB/HwAPABucLiJA/B/wsar2BPoSxN+LiHQA7gcyvDeZCgHGOFtVw2sWwY/nJi9bVXW7qpYBM/HcIyAoqereE3dAU9UCPP+wg3Y4bBHpCPwIeNnpWpwmInHAZcArAKpapqrV3Hg3qIQCUSISCkRTw0CSzUlzCX4b978GIpIK9AOWOluJo54Bfg1UOl1IALgAyAOme7u+XhaRoL3JrqruBp4EdgJ78Qww+YmzVTW85hL8Po/7H0xEJBZ4F5iiqsecrscJInLiXtHLna4lQIQC/YGpqtoPKAKC9pyYd9TgUUAa0B6IEZHbnK2q4TWX4Pd53P9gISJheEJ/hqrOdroeB2UCN4hINp4uwCtF5F/OluSoXcAuVT1xBDgLz4YgWP0A2KGqeapaDswGhjpcU4NrLsH/HdBNRNJEJBzPyZm5DtfkGBERPH24G1T1KafrcZKq/kZVO6pqKp7/L75Q1Wa/R1cTVd0H5IpID++sq4D1DpbktJ3AYBGJ9v67uYogONnt07DMgU5V3SJyL56bvIQA01R1ncNlOSkTuB1YIyIrvfN+q6rzHKzJBI77gBnenaTteO+fEYxUdamIzAJW4Lka7nuC4Fe89stdY4wJMs2lq8cYY4yPLPiNMSbIWPAbY0yQseA3xpggY8FvjDFBxoLfGGOCjAW/McYEGQt+Y4wJMv8fEjn5nyK1cvUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ltrain, label='train_loss')\n",
    "plt.plot(lvalid, label='valid_loss')\n",
    "plt.legend(frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
